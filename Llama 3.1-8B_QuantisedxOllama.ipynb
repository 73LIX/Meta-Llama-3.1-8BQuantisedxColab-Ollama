{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOf4/pPznnp8FAabvu/FnmU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#üöÄRun the new Meta Llama 3.1-8B-Instruct for free with Ollama\n","\n","Meta Llama 3.1 is the latest open source LLM released by meta. It supports 8 Languages `English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai`. Here you can run the model for free in Quantized form provided by `Ollama` on the `T4 Gpu` of google colab.\n","\n","The **Quantized**üéØ form is basically smaller in size then the original model, which saves your disk space, internet and also has faster inferences.\n","\n","Follow the cells from top to bottom, that is from Install Ollama to Run the model.\n","\n","NOTE: This colab notebook performs better than my previous notebook that is the Llama 3.1-8B_Colab. In terms of inference speed and model size this current notebook is better. And you are free from that hugging face access and token hassle.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"YOsRpBmRXCDX"}},{"cell_type":"markdown","source":["| |Google Colab|\n","|:--|:-:|\n","| ‚≠ê **Llama 3.1-8B_Colab** | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/drive/10c_GQ8wqVXuX5JciX0gHVstO0WHaUbqD?usp=sharing )\n","| üåü **Llama 3.1-8B_QuantisedxOllama** |  [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/drive/1S9q6cvH8y2WMml7pczg0Bl-VS6Le-jzZ?usp=sharing)"],"metadata":{"id":"wkrVToR0fzly"}},{"cell_type":"markdown","source":["‚ùóQuantisation can reduce the performance of the model in some use case for example it can make mistakes or create halluciantion, so always check for important info. So if you have the model access on hugging face and also if you have the pro version of google colab then you can also use that."],"metadata":{"id":"ORCHDvPdlUmt"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ArvVhj08drbE"},"outputs":[],"source":["#@title ##Install Ollamaü¶ô\n","!sudo apt-get install -y pciutils\n","!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n","from IPython.display import clear_output\n","clear_output()"]},{"cell_type":"code","source":["#@title ##Start Ollama API Serverüåê\n","import os\n","import threading\n","import subprocess\n","import requests\n","import json\n","\n","def ollama():\n","    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n","    os.environ['OLLAMA_ORIGINS'] = '*'\n","    subprocess.Popen([\"ollama\", \"serve\"])\n","\n","ollama_thread = threading.Thread(target=ollama)\n","ollama_thread.start()"],"metadata":{"cellView":"form","id":"J5Nh3QmSe3cW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title ##Download the model - Meta-Llama-3.1-8B-Instruct‚¨áÔ∏è\n","\n","from IPython.display import clear_output\n","!ollama pull llama3.1:8b\n","!pip install -U lightrag[ollama]\n","!pip install gradio\n","clear_output()"],"metadata":{"cellView":"form","id":"NbE6iq-u4yhW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title ##Run the modelüöÄ\n","#@markdown ‚ùóRun this cell before running the `Gradio UI` cell.\n","\n","from lightrag.core.generator import Generator\n","from lightrag.core.component import Component\n","from lightrag.core.model_client import ModelClient\n","from lightrag.components.model_client import OllamaClient\n","from IPython.display import Markdown, display\n","\n","import time\n","\n","\n","qa_template = r\"\"\"<SYS>\n","You are a helpful assistant.\n","</SYS>\n","User: {{input_str}}\n","You:\"\"\"\n","\n","class SimpleQA(Component):\n","    def __init__(self, model_client: ModelClient, model_kwargs: dict):\n","        super().__init__()\n","        self.generator = Generator(\n","            model_client=model_client,\n","            model_kwargs=model_kwargs,\n","            template=qa_template,\n","        )\n","\n","    def call(self, input: dict) -> str:\n","        return self.generator.call({\"input_str\": str(input)})\n","\n","    async def acall(self, input: dict) -> str:\n","        return await self.generator.acall({\"input_str\": str(input)})\n","\n","model = {\n","    \"model_client\": OllamaClient(),\n","    \"model_kwargs\": {\"model\": \"llama3.1:8b\"}\n","}\n","qa = SimpleQA(**model)\n","users_prompt = \"Hey! What's up?\" #@param {type:\"string\"}\n","output=qa(users_prompt)\n","display(Markdown(f\"**Answer:** {output.data}\"))"],"metadata":{"id":"evdsRFrh5pzH","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title ##Gradio UIüñºÔ∏è\n","import gradio as gr\n","\n","def get_answer(users_prompt):\n","    output = qa.call(users_prompt)\n","    return output.data\n","\n","# Gradio interface\n","gradioUI = gr.Interface(\n","    fn=get_answer,\n","    inputs=gr.Textbox(lines=2, placeholder=\"Chat with Llama here...\"),\n","    outputs=\"text\",\n","    title=\"Run Llama 3.1-8B-Instruct <br> Notebook By <a href='https://github.com/73LIX' target='_blank'>GouravYdv</a>\",\n","    description=\"Llama 3.1 can make mistakes. Check for important info.\"\n",")\n","\n","# Launch\n","gradioUI.launch()"],"metadata":{"cellView":"form","id":"uQexG-M5kG-s"},"execution_count":null,"outputs":[]}]}